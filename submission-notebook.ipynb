{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thealex/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (11,12,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting datetime to years/months/days... ticket_issued_date\n",
      "Converting datetime to years/months/days... hearing_date\n",
      "Converting to categorical... agency_name # variables: 5\n",
      "Converting to categorical... violation_code # variables: 72\n",
      "Converting to categorical... disposition # variables: 8\n",
      "Converting to categorical... grafitti_status # variables: 2\n",
      "Number of features: 97\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Open data files\n",
    "path = \"./data/\"\n",
    "\n",
    "train = pd.read_csv(path+'train.csv', encoding='iso-8859-1')[::]\n",
    "test = pd.read_csv(path+'test.csv')\n",
    "test_ticket_id = np.array(test['ticket_id'])\n",
    "\n",
    "train = train.set_index('ticket_id')\n",
    "test = test.set_index('ticket_id')\n",
    "\n",
    "# Drop the violators who were found not responsible\n",
    "train.dropna(subset=['compliance'], inplace=True)\n",
    "\n",
    "# Drop some uninformative features\n",
    "for column_name in ['inspector_name', 'violator_name',\n",
    "                    'violation_zip_code', 'violation_street_number', 'violation_street_name',\n",
    "                    'mailing_address_str_number', 'mailing_address_str_name', 'city',\n",
    "                    'state', 'zip_code', 'non_us_str_code', 'country',\n",
    "                    'violation_description',\n",
    "                    'admin_fee', 'state_fee', 'late_fee']:\n",
    "    test.drop(column_name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Convert datetime columns into years/months/days\n",
    "for column_name in ['ticket_issued_date', 'hearing_date']:\n",
    "    print('Converting datetime to years/months/days...', column_name)\n",
    "    \n",
    "    # test\n",
    "    day_time = pd.to_datetime(test[column_name])\n",
    "    test.drop(column_name, axis=1, inplace=True)\n",
    "    test[column_name+'_month'] = np.array(day_time.dt.month)\n",
    "    test[column_name+'_year'] = np.array(day_time.dt.year)\n",
    "    test[column_name+'_day'] = np.array(day_time.dt.day)\n",
    "    test[column_name+'_dayofweek'] = np.array(day_time.dt.dayofweek)\n",
    "    \n",
    "    # train\n",
    "    day_time = pd.to_datetime(train[column_name])\n",
    "    train.drop(column_name, axis=1, inplace=True)\n",
    "    train[column_name+'_month'] = np.array(day_time.dt.month)\n",
    "    train[column_name+'_year'] = np.array(day_time.dt.year)\n",
    "    train[column_name+'_day'] = np.array(day_time.dt.day)\n",
    "    train[column_name+'_dayofweek'] = np.array(day_time.dt.dayofweek)\n",
    "\n",
    "# Convert string columns to categorical\n",
    "cols = test.select_dtypes(exclude=['float', 'int']).columns\n",
    "len_train = len(train)\n",
    "temp_concat = pd.concat((train[cols], test[cols]), axis=0)\n",
    "\n",
    "# Some filtering on violation_code to make it more manageable\n",
    "temp_concat['violation_code'] = temp_concat['violation_code'].apply(lambda x: x.split(' ')[0])\n",
    "temp_concat['violation_code'] = temp_concat['violation_code'].apply(lambda x: x.split('(')[0])\n",
    "temp_concat['violation_code'][temp_concat['violation_code'].apply(lambda x: x.find('-')<=0)] = np.nan\n",
    "\n",
    "# Make all codes with < 10 occurrences null\n",
    "counts = temp_concat['violation_code'].value_counts()\n",
    "temp_concat['violation_code'][temp_concat['violation_code'].isin(counts[counts < 10].index)] = np.nan\n",
    "\n",
    "for column_name in cols:\n",
    "    print('Converting to categorical...', column_name, '# variables:', len(temp_concat[column_name].unique()))\n",
    "    dummies = pd.get_dummies(temp_concat[column_name])\n",
    "    temp_concat[dummies.columns] = dummies\n",
    "    temp_concat.drop(column_name, axis=1, inplace=True)\n",
    "    train.drop(column_name, axis=1, inplace=True)\n",
    "    test.drop(column_name, axis=1, inplace=True)\n",
    "\n",
    "train[temp_concat.columns] = temp_concat.loc[train.index]\n",
    "test[temp_concat.columns] = temp_concat.loc[test.index]\n",
    "\n",
    "features = list( test.columns )\n",
    "target = ['compliance']\n",
    "\n",
    "print(\"Number of features:\", len(features))\n",
    "\n",
    "# Train Set\n",
    "X = train[features]\n",
    "y = np.array(train[target]).ravel()\n",
    "\n",
    "# Normalize\n",
    "mn = X.mean()\n",
    "std = X.std()\n",
    "X = (X - mn)/std\n",
    "\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X[pd.isnull(X)] = 0\n",
    "\n",
    "# Submissions Set\n",
    "Xtest = (test[features] - mn) / std\n",
    "Xtest = Xtest.replace([np.inf, -np.inf], np.nan)\n",
    "Xtest[pd.isnull(Xtest)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thealex/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble     import RandomForestClassifier\n",
    "from sklearn.ensemble     import RandomForestRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Add classifiers\n",
    "classifiers = {\n",
    "#     \"ASGD\": SGDClassifier(average=True),\n",
    "    \"SAG\": LogisticRegression(solver='sag', tol=1e-1, C=1.e4 / train[features].shape[0]),\n",
    "    \"RF_C\": RandomForestClassifier(max_depth=25),\n",
    "    \"GradBoost\": GradientBoostingClassifier(learning_rate=0.05, max_depth=10, n_estimators=300)\n",
    "#     \"grad\": GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Create Train/Test split for evaluation.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate with K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "scores = dict()\n",
    "# Select the model\n",
    "for classifier_type in classifiers.keys():\n",
    "    # Train classifier\n",
    "    clf = classifiers[classifier_type]\n",
    "    # Score classifier\n",
    "    model_score = cross_val_score(clf, X_test, y_test, cv=5, n_jobs=4)\n",
    "    # Record score\n",
    "    scores[classifier_type] = model_score\n",
    "\n",
    "scores = pd.DataFrame(data=scores)\n",
    "display(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.stripplot(data=scores)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Comparison of Model Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Select the model\n",
    "# for classifier_type in classifiers.keys():\n",
    "    \n",
    "#     # Train classifier\n",
    "#     clf = classifiers[classifier_type]\n",
    "#     clf.fit(X, y)\n",
    "    \n",
    "#     # Predict\n",
    "#     y_pred = np.array(clf.predict(Xtest))\n",
    "#     y_pred = y_pred - y_pred.min()\n",
    "#     y_pred = y_pred / y_pred.max()\n",
    "\n",
    "#     # Save to CSV\n",
    "#     df = {\"ticket_id\":test_ticket_id, \"compliance\":y_pred}\n",
    "#     df = pd.DataFrame(df, columns=[\"ticket_id\", \"compliance\"])\n",
    "#     df.to_csv(\"./data/submission_%s.csv\"%classifier_type, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(model_name, clf):\n",
    "    # Train classifier\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = np.array(clf.predict(Xtest))\n",
    "    y_pred = y_pred - y_pred.min()\n",
    "    y_pred = y_pred / y_pred.max()\n",
    "\n",
    "    # Save to CSV\n",
    "    df = {\"ticket_id\":test_ticket_id, \"compliance\":y_pred}\n",
    "    df = pd.DataFrame(df, columns=[\"ticket_id\", \"compliance\"])\n",
    "    df.to_csv(\"./data/submission_%s.csv\" % model_name, index=False)\n",
    "\n",
    "def make_all_submissions(classifiers):\n",
    "    \"\"\"classifiers: A dictionary of classifier name keys and sklearn classifier value pairs\"\"\"\n",
    "    for model_name, clf in classifiers.items():\n",
    "        make_submission(model_name, clf)\n",
    "\n",
    "# make_all_submissions(classifiers)\n",
    "make_submission(\"GradBoost\", classifiers[\"GradBoost\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
